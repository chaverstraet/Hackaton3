{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES </b><br><br> \n",
    "<b>Hackathon 03 - Classification: Do I need an umbrella tomorrow?  </b></font> <br><br><br>\n",
    "\n",
    "<font size=5  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br> \n",
    "Cécile Hautecoeur    (cecile.hautecoeur@uclouvain.be)<br> \n",
    "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br> \n",
    "Loïc Van Hoorebeeck  (loic.vanhoorebeeck@uclouvain.be)<br> \n",
    "<div style=\"text-align: right\"> Version 2020</div>\n",
    "\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n",
    "-  This assignment is due on November 30th (Monday) at noon.\n",
    "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <br>**Each source of inspiration (stack overflow, git, other groups,...) must be clearly indicated!**\n",
    "-  The notebook (with the \"ipynb\" extension) file must be delivered on **Moodle**. \n",
    "-  Apart from textual environments, comments, pieces of advice, ... the instructions are highlighted in <b>bold</b> within boxes (see below).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 0] (Warm-Up) Getting in touch with the dataset. </b>  <br>\n",
    "    <b>Read carefully</b> the meaning of the features in the link below; most of them refer to features that will be part of the dataset used in this hackathon.<br> \n",
    "</div> \n",
    "\n",
    "[Link here](http://www.bom.gov.au/climate/dwo/IDCJDW0000.pdf?fbclid=IwAR2xysXOMC6HHZjncUcq0SsFlonusQxvuaduFfYmn7fXEI7Y_IP-X-It3zs)\n",
    "\n",
    "<i>Markdown</i> cells request from you that you explain your reasoning, answer an open question ... in plain text.<br> <i>Code</i> cells must be filled with Python code; during our work review your whole notebook should run all at once without any issue.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font size=5 color=#009999> <b>CONTEXT & NOTEBOOK STRUCTURE</b> </font> <br>\n",
    "    \n",
    "In today's hackathon you will be confronted to a classical data science task, namely a *binary classification* thanks to a [thresholded output](http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/logistic/LogisticRegression.pdf) from a trained [logistic regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf) model.<br><br> \n",
    "\n",
    "<img src=\"Imgs/good_or_bad.jpg\" width = \"400\">\n",
    "\n",
    "More specifically you are given a dataset of records about atmospheric conditions for a couple of locations in Australia, at different timestamps (days) together with the information: <font color=green>Yes</font> or <font color=red>No</font>, *has it rained* the day after the one considered in the timestamp (stored in the feature *RainTomorrow*) ? <br><br>Your ultimate goal resides in the building of the <i>most accurate predictor possible</i> (different measures will be studied) that determines, based on the same atmospheric conditions, ...etc, <b>whether or not people will need an umbrella tomorrow</b>. <br><br> \n",
    "\n",
    "We have divided this notebook to mimic a complete pipeline of common data science procedures. It is organized as follows:\n",
    "* PART 1 - DATA PREPROCESSING\n",
    "   - 1.1 - Import the data\n",
    "   - 1.2 - Split the dataset\n",
    "   - 1.3 - Deal with the NA values\n",
    "    <br><br>\n",
    "* PART 2 - EXPLORATORY DATA ANALYSIS (EDA)\n",
    "    - 2.1 - Feature quality assessment\n",
    "    - 2.2 - One to one relationships fetching\n",
    "   <br><br>\n",
    "* PART 3 - CLASSIFICATION / PERFORMANCE ESTIMATION\n",
    "   - 3.1 - Brief feature selection\n",
    "   - 3.2 - Cross-Validation as performance estimator\n",
    "   \n",
    "We filled this notebook with preliminary (trivial) code #factice. This practice makes possibble to run each cell, even the last ones, without throwing warnings. **Take advantage of this aspect to partion the work between all team's members!** <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import of common utilitary librairies\n",
    "import numpy as np\n",
    "np.random.seed(333)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from scipy import stats\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# about data_test\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7 color=#009999> <b>PART 1 - DATA PREPROCESSING</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>1.1 - IMPORT THE DATA</b> \n",
    "</font> <br> <br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Wait for the signal!</b>  <br>\n",
    "Many of you might already know some standard techniques in order to clean and set up properly a raw dataset. We ask you to wait before applying transformations to the features, records, ... etc because this task will be driven by *instructions* that everybody must follow in the same fashion. <br> \n",
    "Therefore stay calm and patient, your time will come to show the TAs your data science skills in the questions that will arise slightly after.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already learned how to import a dataset (e.g. \"csv\" extension) with the function <samp>read_csv</samp> from the well known <samp>pandas</samp> library.\n",
    "\n",
    "Let us present a simple way to get an informative summary about this dataframe thanks to the function <samp>info</samp>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           142193 non-null  object \n",
      " 1   Location       142193 non-null  object \n",
      " 2   MinTemp        141556 non-null  float64\n",
      " 3   MaxTemp        141871 non-null  float64\n",
      " 4   Rainfall       140787 non-null  float64\n",
      " 5   Evaporation    81350 non-null   float64\n",
      " 6   Sunshine       74377 non-null   float64\n",
      " 7   WindGustDir    132863 non-null  object \n",
      " 8   WindGustSpeed  132923 non-null  float64\n",
      " 9   WindDir9am     132180 non-null  object \n",
      " 10  WindDir3pm     138415 non-null  object \n",
      " 11  WindSpeed9am   140845 non-null  float64\n",
      " 12  WindSpeed3pm   139563 non-null  float64\n",
      " 13  Humidity9am    140419 non-null  float64\n",
      " 14  Humidity3pm    138583 non-null  float64\n",
      " 15  Pressure9am    128179 non-null  float64\n",
      " 16  Pressure3pm    128212 non-null  float64\n",
      " 17  Cloud9am       88536 non-null   float64\n",
      " 18  Cloud3pm       85099 non-null   float64\n",
      " 19  Temp9am        141289 non-null  float64\n",
      " 20  Temp3pm        139467 non-null  float64\n",
      " 21  RainToday      140787 non-null  object \n",
      " 22  RainTomorrow   142193 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 25.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data_full = pd.read_csv('Data/weatherAUS_stud.csv')\n",
    "data_full.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "data_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that the import succeeded let us show the *10* first rows of *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-12-06</td>\n",
       "      <td>Albury</td>\n",
       "      <td>14.6</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>56.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1005.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-12-07</td>\n",
       "      <td>Albury</td>\n",
       "      <td>14.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>50.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1009.6</td>\n",
       "      <td>1008.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>24.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-12-08</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>35.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1013.4</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.3</td>\n",
       "      <td>25.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-12-09</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.7</td>\n",
       "      <td>31.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNW</td>\n",
       "      <td>80.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1008.9</td>\n",
       "      <td>1003.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-12-10</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>28.0</td>\n",
       "      <td>S</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>1005.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.1</td>\n",
       "      <td>28.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "5  2008-12-06   Albury     14.6     29.7       0.2          NaN       NaN   \n",
       "6  2008-12-07   Albury     14.3     25.0       0.0          NaN       NaN   \n",
       "7  2008-12-08   Albury      7.7     26.7       0.0          NaN       NaN   \n",
       "8  2008-12-09   Albury      9.7     31.9       0.0          NaN       NaN   \n",
       "9  2008-12-10   Albury     13.1     30.1       1.4          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "5         WNW           56.0          W  ...        55.0         23.0   \n",
       "6           W           50.0         SW  ...        49.0         19.0   \n",
       "7           W           35.0        SSE  ...        48.0         19.0   \n",
       "8         NNW           80.0         SE  ...        42.0          9.0   \n",
       "9           W           28.0          S  ...        58.0         27.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "5       1009.2       1005.4       NaN       NaN     20.6     28.9         No   \n",
       "6       1009.6       1008.2       1.0       NaN     18.1     24.6         No   \n",
       "7       1013.4       1010.1       NaN       NaN     16.3     25.5         No   \n",
       "8       1008.9       1003.6       NaN       NaN     18.3     30.2         No   \n",
       "9       1007.0       1005.7       NaN       NaN     20.1     28.2        Yes   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "5            No  \n",
       "6            No  \n",
       "7            No  \n",
       "8           Yes  \n",
       "9            No  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>1.2 - SPLIT THE DATASET </b>\n",
    "</font> <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to stay the most statistically significant when it comes to estimate your prediction fitness, one common action to be undertaken at the very beginning of any data science work is the split of the whole dataset into a learning chunk *data* (roughly $90\\%$) and a test chunck *data_test* ($10\\%$ remaining). \n",
    "\n",
    "Let's use the command <samp>train_test_split</samp> from <samp>sklearn.model_selection</samp> that achieves precisely this goal!\n",
    "\n",
    "On this latter the exact same data processing steps will be executed before any prediction task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks to sklearn this task is rather straightforwarfd \n",
    "data,data_test = train_test_split(data_full, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>1.3 - DEAL WITH THE NA VALUES </b>\n",
    "</font> <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to be able to manually identify the features for which there are missing values. We will then come back later to their specific treatment!<br> \n",
    "\n",
    "This is what we have done for you so far: \n",
    "\n",
    "<b>Storing</b> the names of all the columns in a list <i>cols_all</i>, names of the columns with missing values in a list <i>cols_missing_values</i> and, finally, the clean columns in a list <i>cols_complete_values</i>. <br>\n",
    "\n",
    "<b>Printing</b> the length of these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some handy functions user ready\n",
    "\n",
    "\"\"\"\n",
    "@pre: list total, list check\n",
    "@post: returns a list remainder with elements of total that are not in list check\n",
    "\"\"\"\n",
    "def list_difference(total,check):\n",
    "    return [elem for elem in total if elem not in check]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "@pre: \n",
    "\n",
    "data => pandas dataframe \n",
    "feature => string representing a feature of data for which the empirical distributions must be visualized\n",
    "class_column => discriminative feature of data \n",
    "classes => list of possible values (classes) for the feature class_column passed in argument\n",
    "color_scale => simple graphical artifact (list)\n",
    "\n",
    "@post:\n",
    "\n",
    "shows a discriminative boxplot (per class) of the observations about 'feature' stored in 'data'\n",
    "\"\"\"\n",
    "# code from https://info.cambridgespark.com/latest/eda-and-interactive-figures-with-plotly\n",
    "\n",
    "def colored_Box(data, classes, feature, color_scale, class_column):\n",
    "    traces = []\n",
    "    for i in range(len(classes)):\n",
    "        idx = data[class_column] == classes[i]\n",
    "        class_trace = go.Box(\n",
    "            y = data[idx][feature].values,\n",
    "            name=\"class {}\".format(classes[i]),\n",
    "            boxpoints='suspectedoutliers',\n",
    "            boxmean='sd',\n",
    "            marker=dict(\n",
    "                color=color_scale[i],\n",
    "                outliercolor='rgba(255, 221, 23, 0.6)',\n",
    "            )\n",
    "        )\n",
    "        traces.append(class_trace)\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 features in total, including the target.\n",
      "For 20 of them at least one record presents a missing/invalid value.\n",
      "The 3 remaining features are complete.\n"
     ]
    }
   ],
   "source": [
    "# getting the names of the features with missing values \n",
    "cols_all = list(data.columns)\n",
    "cols_missing_values = list(data.columns[data.isna().any()])\n",
    "cols_complete_values = list_difference(cols_all,cols_missing_values)\n",
    "\n",
    "# printing some informations\n",
    "print('There are '+str(len(cols_all))+' features in total, including the target.')\n",
    "print('For '+str(len(cols_missing_values))+' of them at least one record presents a missing/invalid value.')\n",
    "print('The '+str(len(cols_complete_values))+' remaining features are complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.1] Cleaning that messy dataset... </b>  <br>\n",
    "There are plenty of ways to deal with data whose records/columns present invalid or missing values. <br> For the purpose of this hackathon <b><i>2</i> variants of data cleaning</b> are approached. \n",
    "<ol>\n",
    "   <li> <i>Keep it all or drop it all</i>: it simply consists in dropping any record admitting at least one invalid entry. <br><br>\n",
    "   <li> <i>Data rescue by clever filling and pruning</i>: follow the instructions.\n",
    "</ol>    \n",
    "    \n",
    "<b>Implement</b> the second one and <b>cite</b>, for both, at least <i>1</i> pro and <i>1</i> con</b>. <br> \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#7AABD4>**Make sure** to keep a copy of the original version of *data* at each step.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `Keep it all or drop it all`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Another thing for you!</b>  <br>\n",
    "This variant is already implemented; it provides a dataset <i>data_drop_it_all</i> that can be used for Part 2. already. <br><b>Nevertheless</b> it remains to answer the qualitative questions about the pros/cons of that cleaning method.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50772 entries, 64921 to 73708\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           50772 non-null  object \n",
      " 1   Location       50772 non-null  object \n",
      " 2   MinTemp        50772 non-null  float64\n",
      " 3   MaxTemp        50772 non-null  float64\n",
      " 4   Rainfall       50772 non-null  float64\n",
      " 5   Evaporation    50772 non-null  float64\n",
      " 6   Sunshine       50772 non-null  float64\n",
      " 7   WindGustDir    50772 non-null  object \n",
      " 8   WindGustSpeed  50772 non-null  float64\n",
      " 9   WindDir9am     50772 non-null  object \n",
      " 10  WindDir3pm     50772 non-null  object \n",
      " 11  WindSpeed9am   50772 non-null  float64\n",
      " 12  WindSpeed3pm   50772 non-null  float64\n",
      " 13  Humidity9am    50772 non-null  float64\n",
      " 14  Humidity3pm    50772 non-null  float64\n",
      " 15  Pressure9am    50772 non-null  float64\n",
      " 16  Pressure3pm    50772 non-null  float64\n",
      " 17  Cloud9am       50772 non-null  float64\n",
      " 18  Cloud3pm       50772 non-null  float64\n",
      " 19  Temp9am        50772 non-null  float64\n",
      " 20  Temp3pm        50772 non-null  float64\n",
      " 21  RainToday      50772 non-null  object \n",
      " 22  RainTomorrow   50772 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 9.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64921</th>\n",
       "      <td>2015-03-25</td>\n",
       "      <td>MelbourneAirport</td>\n",
       "      <td>10.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>S</td>\n",
       "      <td>31.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1018.6</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>20.9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13570</th>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>Moree</td>\n",
       "      <td>15.4</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>SSE</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>1011.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133956</th>\n",
       "      <td>2011-12-02</td>\n",
       "      <td>AliceSprings</td>\n",
       "      <td>14.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>11.8</td>\n",
       "      <td>ESE</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1013.3</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>30.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64667</th>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>MelbourneAirport</td>\n",
       "      <td>6.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>N</td>\n",
       "      <td>37.0</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>1028.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60241</th>\n",
       "      <td>2010-05-21</td>\n",
       "      <td>Sale</td>\n",
       "      <td>6.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>26.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1023.6</td>\n",
       "      <td>1021.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>14.1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64966</th>\n",
       "      <td>2015-05-09</td>\n",
       "      <td>MelbourneAirport</td>\n",
       "      <td>11.2</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NNW</td>\n",
       "      <td>59.0</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1013.9</td>\n",
       "      <td>1010.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>16.8</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78405</th>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>Watsonia</td>\n",
       "      <td>4.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>E</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1025.5</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116975</th>\n",
       "      <td>2015-06-25</td>\n",
       "      <td>PerthAirport</td>\n",
       "      <td>7.6</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>ENE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1027.7</td>\n",
       "      <td>1024.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60304</th>\n",
       "      <td>2010-07-23</td>\n",
       "      <td>Sale</td>\n",
       "      <td>5.6</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>W</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1034.8</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77376</th>\n",
       "      <td>2013-09-11</td>\n",
       "      <td>Watsonia</td>\n",
       "      <td>9.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>W</td>\n",
       "      <td>43.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1010.9</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>15.1</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date          Location  MinTemp  MaxTemp  Rainfall  Evaporation  \\\n",
       "64921   2015-03-25  MelbourneAirport     10.9     21.9       0.0          6.2   \n",
       "13570   2013-12-03             Moree     15.4     34.1       0.0         10.0   \n",
       "133956  2011-12-02      AliceSprings     14.3     32.5       0.0         13.8   \n",
       "64667   2014-07-14  MelbourneAirport      6.6     12.4       0.4          0.8   \n",
       "60241   2010-05-21              Sale      6.8     15.4       1.8          0.4   \n",
       "64966   2015-05-09  MelbourneAirport     11.2     18.9       0.0          3.0   \n",
       "78405   2016-07-09          Watsonia      4.6     14.7       0.0          1.4   \n",
       "116975  2015-06-25      PerthAirport      7.6     20.4       0.0          2.4   \n",
       "60304   2010-07-23              Sale      5.6     13.2       1.0          0.4   \n",
       "77376   2013-09-11          Watsonia      9.1     17.0       0.0          3.4   \n",
       "\n",
       "        Sunshine WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  \\\n",
       "64921        6.3           S           31.0          W  ...        56.0   \n",
       "13570       13.4         SSE           37.0        NNE  ...        49.0   \n",
       "133956      11.8         ESE           50.0        ESE  ...        13.0   \n",
       "64667        2.1           N           37.0          N  ...        94.0   \n",
       "60241        4.0         ESE           26.0        WNW  ...        96.0   \n",
       "64966        7.2         NNW           59.0          N  ...        69.0   \n",
       "78405        7.8           E           15.0        NNE  ...       100.0   \n",
       "116975       9.2         ENE           30.0         NE  ...        64.0   \n",
       "60304        1.9           W           33.0        NNW  ...        92.0   \n",
       "77376        6.4           W           43.0        WNW  ...        63.0   \n",
       "\n",
       "        Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  \\\n",
       "64921          38.0       1018.6       1013.5       7.0       7.0     12.3   \n",
       "13570          15.0       1016.5       1011.1       0.0       1.0     22.8   \n",
       "133956         10.0       1013.3       1008.7       1.0       5.0     24.1   \n",
       "64667          62.0       1033.0       1028.4       7.0       7.0      8.4   \n",
       "60241          58.0       1023.6       1021.6       7.0       5.0      9.9   \n",
       "64966          56.0       1013.9       1010.9       1.0       6.0     14.4   \n",
       "78405          66.0       1025.5       1023.2       2.0       5.0      7.3   \n",
       "116975         35.0       1027.7       1024.2       0.0       1.0     12.1   \n",
       "60304          70.0       1034.8       1032.0       7.0       6.0      8.6   \n",
       "77376          45.0       1010.9       1011.0       1.0       7.0     12.8   \n",
       "\n",
       "        Temp3pm  RainToday  RainTomorrow  \n",
       "64921      20.9         No            No  \n",
       "13570      32.0         No            No  \n",
       "133956     30.6         No            No  \n",
       "64667      11.2         No            No  \n",
       "60241      14.1        Yes            No  \n",
       "64966      16.8         No           Yes  \n",
       "78405      14.1         No            No  \n",
       "116975     19.9         No            No  \n",
       "60304      12.6         No            No  \n",
       "77376      15.1         No           Yes  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation of a smaller dataframe for which all the records are complete\n",
    "data_drop_it_all = data.dropna()\n",
    "data_drop_it_all.info()\n",
    "\n",
    "# applying the same commands to our test batch \n",
    "data_test_drop_it_all = data_test.dropna()\n",
    "\n",
    "# print header \n",
    "data_drop_it_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CONS OF METHOD 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a loss of information because we drop a part of the data (especially in the other columns). This is especially the case when there is a lot of information of a certain column that is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PROS OF METHOD 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just take verified results, we don't change the values so the data stays correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `Data rescue by clever filling and pruning`\n",
    "\n",
    "Each feature of the list *cols_missing_values* will be reworked: one tends to remain as clever as possible to fill in adequately the holes.\n",
    "\n",
    "We propose the following *easy to start with* treatments. Of course more advanced techniques relying on similarities and subspace projections exist... <br>\n",
    "\n",
    "For your information (this is <b>not</b> what we will implement), let us state:\n",
    "\n",
    "* **KNN** : after defining a suitable metric over the available data, predict the missing informations of a given tuple with the ones of the closest record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Notice </b>  <br>\n",
    "Please <b>note</b> that it might be possible due to the random shufflings (split training dataset/ test dataset) that some of the features mentioned below are actually complete in usual circumstances. However here the random <i>seed</i> has been fixed so that all the comments below are worth consideration.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to keep data untouched, we make a copy\n",
    "data_clever_fill = data.copy()\n",
    "\n",
    "# same for data_test\n",
    "data_test_clever_fill = data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us remove uncritical features which consistent filling seems rather complicated.  After analysis, the effect of the direction of the wind cannot be taken apart from the location (plus eventually the time and other features) in what concerns raining predictability. Excepted if one managed to infer on wind's cardinal/compass value based on a neighbourhood search', we would better drop wind direction related fields.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    Therefore you must <b>cut down</b> the attributes <i>WindDir3pm</i>, <i>WindDir9am</i> and <i>WindGustDir</i>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clever_fill  = data_clever_fill.drop(\"WindDir3pm\", axis=1)\n",
    "data_clever_fill  = data_clever_fill.drop(\"WindDir9am\", axis=1)\n",
    "data_clever_fill  = data_clever_fill.drop(\"WindGustDir\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by fixing the features *MinTemp* and *MaxTemp*. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Replace</b> the missing values within <i>MinTemp</i> (resp. <i>MaxTemp</i>) with the median values of the available temperatures for that feature. <br> We suggest that you look after the command <samp>nanmedian</samp> from the famous <samp>numpy</samp> library or, as during TP1, you may reuse <samp>median</samp> from <samp>pandas</samp>.<br> <br><b>Comment</b> this procedure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianMin = data_clever_fill[\"MinTemp\"].median()\n",
    "medianMax = data_clever_fill[\"MaxTemp\"].median()\n",
    "data_clever_fill[\"MinTemp\"] = data_clever_fill[\"MinTemp\"].fillna(medianMin)\n",
    "data_clever_fill[\"MaxTemp\"] = data_clever_fill[\"MaxTemp\"].fillna(medianMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replaced the missing values in the columns with the median of the data we had. This allows us to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori the feature *RainToday* should be informative about the remaining other features like *Sunshine*, *Rainfall* and *Evaporation*... and even the target, namely *RainTomorrow*. Thus we can backtrack from this feature (which's almost complete, there are only a few missing values) in order to infer on the cited related attributes above. \n",
    "\n",
    "**DIRECT BACKTRACKING INFERENCE**: \n",
    "\n",
    "For instance, let us describe what we can do for *Sunshine*. Before anything else, we propose to manually encode values for *RainToday*. We simply fill-in NA with the string <i>Unknown</i>. It makes us $3$ classes for *RainToday* (namely, <font color=green>Yes</font>, <font color=red>No</font> and finally <font color=black>Unknown</font>). Then we predict, when it is non-available for a record, the class median value of *Sunshine* among the instances for which *RainToday* matches record's one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Example:</b> We invite you to have a look at the descriptive picture below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imgs/dbi.png\" width = \"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Apply</b> the Direct Bactraking Inference procedure for every <i>numerical</i> feature that is not complete and which has not been reworked yet.<br> E.g. <b> you shouldn't work on </b> <i>MinTemp</i> and <i>MaxTemp</i>.<br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14220 entries, 20078 to 102228\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           14220 non-null  object \n",
      " 1   Location       14220 non-null  object \n",
      " 2   MinTemp        14163 non-null  float64\n",
      " 3   MaxTemp        14187 non-null  float64\n",
      " 4   Rainfall       14078 non-null  float64\n",
      " 5   Evaporation    8139 non-null   float64\n",
      " 6   Sunshine       7426 non-null   float64\n",
      " 7   WindGustDir    13328 non-null  object \n",
      " 8   WindGustSpeed  13334 non-null  float64\n",
      " 9   WindDir9am     13259 non-null  object \n",
      " 10  WindDir3pm     13843 non-null  object \n",
      " 11  WindSpeed9am   14089 non-null  float64\n",
      " 12  WindSpeed3pm   13957 non-null  float64\n",
      " 13  Humidity9am    14050 non-null  float64\n",
      " 14  Humidity3pm    13870 non-null  float64\n",
      " 15  Pressure9am    12852 non-null  float64\n",
      " 16  Pressure3pm    12863 non-null  float64\n",
      " 17  Cloud9am       8861 non-null   float64\n",
      " 18  Cloud3pm       8545 non-null   float64\n",
      " 19  Temp9am        14135 non-null  float64\n",
      " 20  Temp3pm        13955 non-null  float64\n",
      " 21  RainToday      14220 non-null  object \n",
      " 22  RainTomorrow   14220 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# fill-in records with missing values for the field 'RainToday'\n",
    "data_clever_fill['RainToday'].fillna('Unknown',inplace=True)\n",
    "\n",
    "#same for test data\n",
    "data_test_clever_fill['RainToday'].fillna('Unknown',inplace=True)\n",
    "\n",
    "\n",
    "# quick check that the unique values for the feature 'RainToday' are Yes, No and Unknown\n",
    "data_clever_fill.RainToday.unique()\n",
    "\n",
    "data_test_clever_fill.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @write the body of the function backTrackMedian\n",
    "\n",
    "'''\n",
    "@pre: \n",
    "\n",
    "dataframe => the dataframe (pandas dataframe) we are working on\n",
    "\n",
    "                        e.g. dataframe = data_clever_ill\n",
    "\n",
    "list_inferred_features => list of NUMERICAL features (str) to be inferred according to the backTrackMedian policy\n",
    "                                        \n",
    "                        e.g. list_inferred_features = ['Rainfall','Evaporation','Sunshine', ....]\n",
    "                        \n",
    "discriminative_feature => (complete) discriminative feature of the backTrackMedian policy (str)\n",
    "\n",
    "                        e.g. discriminative_feature = 'RainToday'\n",
    "\n",
    "@post: \n",
    "\n",
    "for each feature present in 'list_inferred_features', fill-in the NA values based on the Direct Backtracking Inference\n",
    "methodology\n",
    "\n",
    "/!\\ IF A FEATURE FROM THE LIST list_inferred_features IS SUCH THAT FOR A CLASS THERE IS ABSOLUTELY \n",
    "    NO RECORD WITH A NON-NA VALUE, REPLACE ALL THE NA ENTRIES FOR THAT CLASS WITHIN THIS FEATURE \n",
    "    WITH THE MEAN OF THE AVAILABLE OTHER 'PER CLASS' MEDIANS FOR THAT FEATURE /!\\ \n",
    "    \n",
    "e.g. you should normally observe that for the numerical feature Rainfall, every record whose class for \n",
    "the discriminative feature RainToday is set to Unknown present a NA (for Rainfall), then fill-in these NA\n",
    "with the mean of the median values of Rainfall for the classes Yes and No.\n",
    "\n",
    "NOTE:: the method should not return anything but should modify the object 'dataframe' in place\n",
    "\n",
    "here, since you should pass 'data_clever_fill' in argument, you can easily check whether your transformations\n",
    "were correctly taken into account by looking at the result of the command data_clever_fill.info()\n",
    "'''\n",
    "\n",
    "def backTrackMedian(dataframe,list_inferred_features,discriminative_feature):\n",
    "    liste = dataframe[str(discriminative_feature)].unique()   #Liste: [\"yes\", \"no\", unknown]\n",
    "    for i in range(len(list_inferred_features)):              #list_inferred_features: [Sunshine, Evaporation, Rainfall]\n",
    "        median = []\n",
    "        for j in range(len(liste)):\n",
    "            subset = dataframe[dataframe[discriminative_feature] == liste[j]][list_inferred_features[i]]\n",
    "            median.append(subset.median())\n",
    "\n",
    "        for k in range(len(liste)):\n",
    "            local = dataframe.loc[dataframe[discriminative_feature] == liste[k],[list_inferred_features[i]]]\n",
    "\n",
    "            if (np.isnan(median[k])):\n",
    "                med = np.nanmean(median)\n",
    "                dataframe.loc[(dataframe[discriminative_feature] == liste[k])&(dataframe[list_inferred_features[i]].isna()),list_inferred_features[i]] = med\n",
    "\n",
    "\n",
    "            else:\n",
    "                dataframe.loc[(dataframe[discriminative_feature] == liste[k])&(dataframe[list_inferred_features[i]].isna()),list_inferred_features[i]] = median[k]\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 127973 entries, 64921 to 73708\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           127973 non-null  object \n",
      " 1   Location       127973 non-null  object \n",
      " 2   MinTemp        127973 non-null  float64\n",
      " 3   MaxTemp        127973 non-null  float64\n",
      " 4   Rainfall       127973 non-null  float64\n",
      " 5   Evaporation    127973 non-null  float64\n",
      " 6   Sunshine       127973 non-null  float64\n",
      " 7   WindGustSpeed  119589 non-null  float64\n",
      " 8   WindSpeed9am   126756 non-null  float64\n",
      " 9   WindSpeed3pm   125606 non-null  float64\n",
      " 10  Humidity9am    126369 non-null  float64\n",
      " 11  Humidity3pm    124713 non-null  float64\n",
      " 12  Pressure9am    115327 non-null  float64\n",
      " 13  Pressure3pm    115349 non-null  float64\n",
      " 14  Cloud9am       79675 non-null   float64\n",
      " 15  Cloud3pm       76554 non-null   float64\n",
      " 16  Temp9am        127154 non-null  float64\n",
      " 17  Temp3pm        125512 non-null  float64\n",
      " 18  RainToday      127973 non-null  object \n",
      " 19  RainTomorrow   127973 non-null  object \n",
      "dtypes: float64(16), object(4)\n",
      "memory usage: 25.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14220 entries, 20078 to 102228\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           14220 non-null  object \n",
      " 1   Location       14220 non-null  object \n",
      " 2   MinTemp        14163 non-null  float64\n",
      " 3   MaxTemp        14187 non-null  float64\n",
      " 4   Rainfall       14220 non-null  float64\n",
      " 5   Evaporation    14220 non-null  float64\n",
      " 6   Sunshine       14220 non-null  float64\n",
      " 7   WindGustDir    13328 non-null  object \n",
      " 8   WindGustSpeed  13334 non-null  float64\n",
      " 9   WindDir9am     13259 non-null  object \n",
      " 10  WindDir3pm     13843 non-null  object \n",
      " 11  WindSpeed9am   14089 non-null  float64\n",
      " 12  WindSpeed3pm   13957 non-null  float64\n",
      " 13  Humidity9am    14050 non-null  float64\n",
      " 14  Humidity3pm    13870 non-null  float64\n",
      " 15  Pressure9am    12852 non-null  float64\n",
      " 16  Pressure3pm    12863 non-null  float64\n",
      " 17  Cloud9am       8861 non-null   float64\n",
      " 18  Cloud3pm       8545 non-null   float64\n",
      " 19  Temp9am        14135 non-null  float64\n",
      " 20  Temp3pm        13955 non-null  float64\n",
      " 21  RainToday      14220 non-null  object \n",
      " 22  RainTomorrow   14220 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# @write your code in this cell, basically you just have to choose the right list of features yet to be reworked :) \n",
    "\n",
    "list_inferred_features = ['Evaporation','Sunshine','Rainfall']\n",
    "\n",
    "backTrackMedian(data_clever_fill,list_inferred_features,'RainToday')\n",
    "\n",
    "# same for test data\n",
    "backTrackMedian(data_test_clever_fill,list_inferred_features,'RainToday')\n",
    "data_clever_fill.info()\n",
    "\n",
    "data_test_clever_fill.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CONS OF METHOD 2\n",
    "\n",
    "(Hint: think about extreme cases (sometimes caused an unfortunate random outcome from the shufflings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second variant creates data, so there is a big change that it does not follow the distribution of the given values. That can create some important bias. For example, if we are missing a value for 1 july and 1 december, it is possible that we fill it with the same value. In reality however, we know it is unlikely they have the same temperature.\n",
    "\n",
    "In the case where we don't have a lot of data for a feature (e.g. 3 values for a column of 10 000), we use approximations that are only based on a small amount of data. That could cause big errors if for example they are outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PROS OF METHOD 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, where the given values are more balanced, it is a good method because it is possible to consider all the other values of the column as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the results and ensure that the dataset is clean now ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clever_fill.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clever_fill.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.2] One must operate a decisive choice! </b>  <br>\n",
    "Choose one variant</b> that will produce the dataset <i>data_sharp</i> you will be working with for the rest of this work.<br> Again, <b>comment / motivate</b> your choice!\n",
    "</div>\n",
    "<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CHOICE OF VARIANT + COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second variant (data rescue by clever filling and pruning) is the best choice when there is enough data to begin with. Otherwise, in most cases it is better to use the first method (keep it all or drop it all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit according to your choice, default choice : data_sharp = data_drop_it_all (variant 1)\n",
    "\n",
    "# you wish to continue with variant 1\n",
    "data_sharp = data_drop_it_all.copy()\n",
    "data_test_sharp = data_test_drop_it_all.copy()\n",
    "\n",
    "# you wish to continue with variant 2\n",
    "#data_sharp = data_clever_fill.copy()\n",
    "#data_test_sharp = data_test_clever_fill.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7 color=#009999> <b>PART 2 - EXPLORATORY DATA ANALYSIS</b> </font> <br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to dig into *data* : digits and several sources of information will help us in our main classification task ! \n",
    "\n",
    "Any data science work starts with a so called **Exploratory Data Analysis (EDA)** where interactions and correlations between features are investigated. <br>One also looks after imbalances in the data provided, structures, low-dimensional embeddings... etc. \n",
    "\n",
    "Let us summarize the *2* main objectives one must achieve at the end of present's hackathon **EDA**.\n",
    "\n",
    "<ol>\n",
    "<li>Feature quality assessment, meaning understanding</li>\n",
    "<li>One-to-One relationships between the features fetching</li>\n",
    "</ol>\n",
    "\n",
    "Despite the fact that is not deepened here, *2* usual side tasks are highlighted below:\n",
    "\n",
    "<ol>  \n",
    "<li>Intrinsic structure catching (not covered)</li>\n",
    "<li>Imbalances spotting </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>2.1 - FEATURE QUALITY ASSESSMENT </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you should dispose of a dataset *data_sharp* which is sound and complete. <br>\n",
    "\n",
    "We want to have a brief glance at each feature distribution, grouped by target class *RainTomorrow*. In order to assess *statistically* the *a priori* importance of each numerical feature we propose to **perform [Welch's T-](https://en.wikipedia.org/wiki/Welch%27s_t-test)tests**. It will be furthermore asked to **explain** in what consists these tests and under which assumptions they make sense. Follow carefully the instructions given in the yellow boxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.1] Target proportions, please? </b>  <br><br>\n",
    "<b>Plot</b> a pie chart representing the amount (or proportion) of records of <i>data_sharp</i> with target <font color='green'>Yes</font> and <font color='red'>No</font> in what concerns the feature <i>RainTomorrow</i>.<br>\n",
    "A sample code has been nicely provided. Your task resumes in editing the code in order to apply it to <i>data_sharp</i>. <br><br>\n",
    "If the data from the test set followed the same proportions regarding the targets, what would be the minimal appreciable <i>empirical accuracy</i> for a classifier on that test set? Why? <b>Answer</b> both questions right after your graph. The empirical accuracy is the percentage of records from our test set for which our machine learning program would predict correctly (<i>without</i> getting that information beforehand of course!) <i><b>\"whether or not it rained tomorrow?\"</b></i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PIE CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @edit with the data from data_sharp\n",
    "labels = ['Oxygen','Hydrogen','Carbon_Dioxide','Nitrogen']\n",
    "values = [4500, 2500, 1053, 500]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n",
    "fig.update_traces(marker=dict(colors=['green','red','blue','yellow']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ANSWERS TO QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.2] Statistical tests... but what for? </b>  <br><br>\n",
    "    <b>Explain</b> the utility of Welch's T test in the context of feature information assessment.   <br>\n",
    "    Considering a numerical feature, we invite you to think about the conclusions one could draw if the <i>p-value</i> of a Welch T test was very small (typically $< 10^{-2}$) while taking as \n",
    "    <ol> \n",
    "        <li> Sample 1: the values taken by the feature for records in target class <font color='green'>Yes</font></li>\n",
    "        <li> Sample 2: the values taken by the feature for records in target class <font color='red'>No</font></li>\n",
    "    </ol>\n",
    "    \n",
    "<b>We strongly advise that you spend time on this question as it conditions your understanding for the next question.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">EXPLANATION OF THE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.3] Are my features <i>a priori</i> informative? </b>  <br><br>\n",
    "    For each numerical feature, <b>develop a tool</b> that produces \n",
    "<ol>\n",
    "    <li> A brief summary of descriptive statistics for the whole feature (not grouped by target class)</li>\n",
    "    <li> A comparative separated boxplot for class <font color='green'>Yes</font> and <font color='red'>No</font> \n",
    "         together with descriptive statistics within each class</li>\n",
    "    <li> Finally, using that same class separation, a Welch's T-test</li>\n",
    "</ol>\n",
    "    \n",
    "Most of the code snippets have already been written, you must <b>adapt the arguments</b> of the different functions in order to produce the desired output!<br>\n",
    "    The main routines involved in this question are <samp>describe</samp> and <samp>ttest_ind</samp> from <samp>stats</samp>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy dataset for experiments ;) , you can delete this cell once you are finished with your work\n",
    "diabetes = pd.read_csv('Data/diabetes.csv')\n",
    "diabetes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Example</b>: <br>Discriminative Target : <i>Outcome</i> <br>\n",
    "One Numerical Feature : <i>BloodPressure</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @write your code in this cell, adapt the code wherever you find an @edit marker\n",
    "\n",
    "df = diabetes.copy() # @edit with df as data_sharp at the end of the day => df = data_sharp.copy()\n",
    "\n",
    "final_numeric_cols_all = df.select_dtypes(include=np.number).columns.tolist()\n",
    "discriminative = 'Outcome' # @edit with the discriminative feature of data_sharp \n",
    "values_disc = df[discriminative].unique()\n",
    "final_numeric_cols_all = list_difference(final_numeric_cols_all,[discriminative])\n",
    "\n",
    "myFeatures = widgets.SelectMultiple(\n",
    "    options= final_numeric_cols_all,\n",
    "    value = [final_numeric_cols_all[0]],\n",
    "    description='Features',\n",
    "    rows = 6,\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @complete the body of the function single analysis where @edit markers are displayed, please refer to @post\n",
    "\n",
    "\"\"\"\n",
    "@pre:\n",
    "df => pandas dataframe you work on (e.g. df <- diabetes by default but you must change that above :) )\n",
    "common_num_feature => feature (str) from df for which a box plot, summary statistics and a Welch T test must be applied\n",
    "discriminative => feature (str) from df that will determine the classes (e.g. 'Outcome' for diabetes but RainTomorrow for data_sharp)\n",
    "values_disc => list of possible values (classes) of the feature discriminative \n",
    "\n",
    "@post: \n",
    "does not return anything but produces the desired content for Question 2.3) \n",
    "\"\"\"\n",
    "\n",
    "def single_analysis(df,common_num_feature,discriminative,values_disc):\n",
    "    \n",
    "    sample = df[common_num_feature].values    \n",
    "    samples1,samples2 = df[df[discriminative]==values_disc[0]][common_num_feature],df[df[discriminative]==values_disc[1]][common_num_feature]\n",
    "    \n",
    "    print(' ')\n",
    "    print(' => Descriptive Statistics (whole feature + by class)')\n",
    "    print(' ----------------------------------------------------')\n",
    "    print(' ')\n",
    "    print('overall stats. from data')\n",
    "    \n",
    "    # @edit here implement the search for descriptive statistics using stats.describe()\n",
    "    \n",
    "    print(' ')\n",
    "    \n",
    "    for value in values_disc:\n",
    "        print('overall stats. from data for '+str(value)+' instances')\n",
    "        # @edit here implement the search for descriptive statistics but only for \n",
    "        # records within the class value \n",
    "        print( '')\n",
    "\n",
    "    box_plot_traces = colored_Box(\n",
    "        data = df,\n",
    "        classes = values_disc,\n",
    "        feature = common_num_feature,\n",
    "        color_scale = ['green','red','blue','black','yellow','orange','purple','magenta'][:len(values_disc)],\n",
    "        class_column = discriminative\n",
    "\n",
    "    )\n",
    "    \n",
    "    fig = dict(data = box_plot_traces)\n",
    "    iplot(fig) \n",
    "    \n",
    "    welch_p_value = 1e-1 # @edit here implement a Welch T test with samples1, samples2 and get the p-value \n",
    "    \n",
    "    print(' ')\n",
    "    print('STATEMENT => @edit with a conclusion <= wrong with (estimated) probability : p = '+\"{:.2e}\".format(welch_p_value))\n",
    "\n",
    "def multiple_analysis(df,list_common_num_features,discriminative,values_disc):\n",
    "    for common_num_feature in list_common_num_features:\n",
    "        single_analysis(df,common_num_feature,discriminative,values_disc);\n",
    "\n",
    "def interactive_analysis(list_common_num_features):\n",
    "    multiple_analysis(df,list_common_num_features,discriminative,values_disc);\n",
    "        \n",
    "interact(interactive_analysis,list_common_num_features=myFeatures);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.4] Do I better visualize which features matter? </b>  <br><br>\n",
    "    Based on the results you obtained above, <b>write</b> the names of some <i>a priori</i> important features and <b>justify</b> your claims.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">COMMENT RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>2.2 - ONE-TO-ONE RELATIONSHIPS FETCHING </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation plots can give quick insights about pairwise (affine) relationships between features. <br>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.5] Quick look at one-to-one correlations </b>  <br><br>\n",
    "    Based on the code provided below, <b>provide</b> a correlation plot and <b>comment</b> the results. <br>More specifically we intend to spot correlations that are, in absolute value, bigger than a certain threshold $\\rho = 0.6$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @write your code in this cell, adapt the code wherever you find an @edit marker\n",
    "\n",
    "corr = diabetes[final_numeric_cols_all].corr() # @edit to make it work with data_sharp \n",
    "\n",
    "abs_threshold = 1.5e-1 # @edit to pick up the right correlation threshold\n",
    "\n",
    "print('correlations above threshold between: ')\n",
    "print('-------------------------------------')\n",
    "for i in np.arange(len(final_numeric_cols_all)):\n",
    "    buff = corr.iloc[i,i+1:]\n",
    "    interest = buff[np.abs(buff)>abs_threshold]\n",
    "    for elem in interest.index:\n",
    "        print(final_numeric_cols_all[i]+' & '+elem+' | rho = '+str(interest[elem]))\n",
    "        \n",
    "plt.figure(figsize=(12,9))\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.6] And now? </b>  <br><br>\n",
    "Based on the above results would it be reasonable to discard some features due their redundancy ? <br>\n",
    "    Briefly <b>explain</b> which features you would prune at this stage and <b>why</b>.<br>\n",
    "    Then, just <b>DO</b> it!\n",
    "</div>\n",
    "\n",
    "Please note that, normally, you should keep <i>RainToday</i>..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @adapt with the results obtained\n",
    "toDiscard = ['WindSpeed9am']\n",
    "\n",
    "data_sharp.drop(columns=toDiscard,inplace=True)\n",
    "\n",
    "# applying the same commands to our test batch \n",
    "data_test_sharp.drop(columns=toDiscard,inplace=True)\n",
    "\n",
    "# update cols_all\n",
    "cols_all = list_difference(cols_all,toDiscard)\n",
    "final_numeric_cols_all = data_sharp.select_dtypes(include=np.number).columns.tolist()\n",
    "final_numeric_cols_all = list_difference(final_numeric_cols_all,['RainToday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7 color=#009999> <b>PART 3 - CLASSIFICATION / PERFORMANCE ESTIMATION </b> </font> <br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <i>Encode string variables</i> </font> <br> <br>\n",
    "\n",
    "Most algorithms are not able to deal categoric variables, and need the data to be integer or float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## /!\\ only useful if data_sharp is data_drop_it_all /!\\\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "enc = OrdinalEncoder()\n",
    "toEncode = data_sharp.select_dtypes(object)\n",
    "toEncode = enc.fit_transform(toEncode)\n",
    "data_sharp[data_sharp.select_dtypes(object).columns] = toEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>3.1 - BRIEF FEATURE SELECTION </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike explicit programming, machine learning is a form of AI that enables a system to learn from directly from data. Modern day datasets are very rich in information with data collected from millions of IoT devices and sensors. This makes the data high dimensional and it is quite common to see datasets with hundreds of features. <br>\n",
    "\n",
    "Feature Selection/Extraction is a very critical component in a Data Scientist’s workflow. When presented data with very high dimensionality, models usually choke because:\n",
    "   - Training time increases exponentially with number of features. Ressources need also to be allocated for uninformative features. \n",
    "   - Models have increasing risk of overfitting with increasing number of features (curse of dimensionality). Uninformative features then act as noise for the machine learning model that can perform terribly poorly.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Feature selection</b> is \"the process of selecting subset of relevant features for processing, without any transformation\". Such methods consider the relationship between features and the target variable to compute the importance of features. \n",
    "</div>\n",
    "\n",
    "There exist plenty of ways to assign an importance score to a given subset of features. We will stick to the most easy one consisting in computing the **[Point-biserial correlation coefficient](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient)** of each remaining numerical feature with respect to the *categorical* target *RainTomorrow*. <br><br><b><font color='purple'>We will not quantify the categorical feature *RainToday* as we intend to keep it anyway.</font></b> Its score will be arbitrarily set to $1$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.1] Quick look at one-to-one correlations </b>  <br><br>\n",
    "    <b>Rank</b> in descending order each feature according to its point-biserial correlation coefficient with <i>RainTomorrow</i>. That is <b>store</b> their names in a Python list, the first entry must be <i>RainToday</i> and then the order will be determined by the susmentionned scores. Make use of the function <samp>pointbiserial</samp> from the package <samp>stats</samp>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HERE BELOW WE GIVE YOU SOME HANDFUL FUNCTIONS\n",
    "'''\n",
    "\n",
    "# convert slabels Yes and No as numerical 1 and 0 for sklearn purpose\n",
    "def labels_to_numerical(labels):\n",
    "    buf = []\n",
    "    for label in labels:\n",
    "        if label=='Yes':\n",
    "            buf.append(1)\n",
    "        else:\n",
    "            buf.append(0)\n",
    "    return buf\n",
    "\n",
    "labels= labels_to_numerical(data_sharp['RainTomorrow'])\n",
    "labels_test = labels_to_numerical(data_test_sharp['RainTomorrow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation of the feature scores, factice code here with random scores @edit\n",
    "myData = [['RainToday',1]]\n",
    "for feature in final_numeric_cols_all:\n",
    "    feature_point_biserial_score = np.random.uniform() # @edit: compute the point biserial correlation asked\n",
    "    myData.append([feature,feature_point_biserial_score])\n",
    "    \n",
    "rankedFeatures = pd.DataFrame(data=myData,columns=['Feature','Score'])\n",
    "rankedFeatures.set_index('Score',inplace=True)\n",
    "rankedFeatures.sort_index(ascending=False,inplace=True)\n",
    "rankedFeatures.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of this hackathon will the model fitting take place. We would like to keep at most $10$ features in our dataframe. <br><br> If your list above contains more than $10$ features, only the $10$ most correlated features to the target are kept in <i>data_sharp</i>. Else, nothing is done..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10\n",
    "ranked_features_list = rankedFeatures['Feature'].values\n",
    "\n",
    "if len(ranked_features_list)>limit:\n",
    "    data_sharp = data_sharp[ranked_features_list[:limit]]\n",
    "    \n",
    "data_sharp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>3.2 - CROSS-VALIDATION AS PERFORMANCE ESTIMATOR </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. \n",
    "\n",
    "- This situation represents the ultimate example of what is usually called **overfitting**. \n",
    "\n",
    "To evaluate the performance of any machine learning model we need to test it on some unseen data. Cross validation (CV) is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data. To perform CV we need to keep aside a sample/portion of the data on which is do not use to train the model, later us this sample for validating. \n",
    "\n",
    "K-Fold CV is where a given (learning) dataset is partioned into a $K$ number of sections/folds where each fold is used as a testing set at some point.<br> Let us take the scenario of $5$-Fold cross validation ($K=5$). Here, the data set is split into $5$ folds. In the first iteration, the first fold is used to test the model (according to a certain accuracy measure pre-defined) and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.\n",
    "\n",
    "<img src=\"Imgs/K-fold.png\" width = \"900\">\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>K-Fold</b>  <br>\n",
    "To implement a K-fold CV in python, use the KFold object (in <samp>klearn.model_selection</samp>) with $K = 10$ folds, and its indices-splitting function split. One call to this function will generate $K$ pairs of lists of indices. In each pair, the first indicates the indices used for training set, and the second gives the indices used for the test set.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "To help you, here is a pseudo algorithm of K-fold in which you just have to substitute the proper functions for each model (see subsequent sections, you will use a K-fold cross-validation for 3 classification algorithms described later):\n",
    "\n",
    "<img src=\"Imgs/K-fold_pseudo-code.png\" width = \"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.2] K-CV implementation </b>  <br><br>\n",
    "    <b>Implement</b> a $K=10$-folds Cross-Validation in order to estimate the <i>F1</i> accuracy measure of a trained predictor (Logistic Regression as announced in the main instructions) on our cleaned and worked-out <i>data_sharp</i>. The estimate you <b>provide</b> should be the mean <i>F1</i> score over the $K$ folds. \n",
    "<br>\n",
    "<br>\n",
    "    <b>Give</b> 1 pro and 1 con of the <i>F1</i> accuracy measure.\n",
    "    <br> <br>\n",
    "<b>Note that the recall and precision must be taken with respect to the class 'Yes' as True Positive !</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "(see [Wikipedia](https://en.wikipedia.org/wiki/F1_score) for further informations about this accuracy metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Imgs/confusionbis.png\" width = \"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HERE BELOW WE GIVE YOU SOME HANDFUL FUNCTIONS\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# converts a vector of values from 0 to 1 to a label Yes or No considering the threshold that is provided\n",
    "def pred_probas_to_pred_labels(probs_vec,threshold=0.5):\n",
    "    buf = []\n",
    "    for prob in probs_vec:\n",
    "        if prob>threshold:\n",
    "            buf.append('Yes')\n",
    "        else:\n",
    "            buf.append('No')\n",
    "    return buf\n",
    "\n",
    "# based on the confusion matrix, computes the 'recall'\n",
    "def cm_to_recall(confusion_matrix):\n",
    "    return confusion_matrix[1,1]/np.sum(confusion_matrix[1,:])\n",
    "\n",
    "\n",
    "# @edit , same as above but computes the 'precision'\n",
    "def cm_to_precision(confusion_matrix):\n",
    "    print('to edit')\n",
    "    return 0\n",
    "\n",
    "# @edit overall F1 measure computation for the chosen threshold based on the output of our predictor\n",
    "def probas_to_F1(probs_vec,true_labels,threshold=0.5):\n",
    "    predicted_labels = probas_to_labels(probs_vec,threshold)\n",
    "    cm = confusion_matrix(true_labels,predicted_labels)\n",
    "    print('to edit')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code sample for model training\n",
    "\n",
    "# here is how to simply fit a logistic regression model based on a dataframe of features X and with corresponding numerical targets in y  \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# taking again the 'diabetes' example from Part 2, Outcome is the target (already in numeric 1 vs. 0)\n",
    "X, y = diabetes.drop(columns=['Outcome']).values,diabetes['Outcome'].values\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "# predict the label (diabete or not diabete) for the two first tuples of X\n",
    "pred_probas = clf.predict_proba(X[:2, :])[:,1]\n",
    "print('predicted')\n",
    "print(pred_probas_to_pred_labels(pred_probas,threshold=0.5))\n",
    "print('true labels')\n",
    "print(pred_probas_to_pred_labels(y[:2],threshold=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full $K$-CV should be written here below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @implement here a function or just some code of lines in order to produce the estimate of the F1 score of \n",
    "# a logistic regression based classifier that 'learns' from data_sharp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ESTIMATION F1 SCORE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PRO F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CON F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.3] Comparison </b>  <br><br>\n",
    "Finally, <b>compute</b> the <i>F1</i> score of a model trained on the whole dataset <i>data_sharp</i> with respect to the test set we have kept from the very beginning of the hackaton. <b>Compare</b> and <b>comment</b> the results against your performance estimate in 3.2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">COMPARISON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@write your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[BONUS] INVESTIGATION OF THE THRESHOLD ? </b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
